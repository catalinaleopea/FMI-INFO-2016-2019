{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"_2wy0l5wBWF7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\"\"\"Laborator GAN.\n","   Wasserstein GAN aplicat pe MNIST.\"\"\"\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.contrib import layers\n","from tensorflow.examples.tutorials.mnist import input_data\n","import os\n","from IPython import display\n","%matplotlib inline\n","import time"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Yb5IqIrkp1A","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Flag pentru train from scratch vs resume\n","resume = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HWaymVuHBmr6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Generare sesiune\n","session = tf.InteractiveSession()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yO-YEb1iSPIo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def leaky_relu(x):\n","  return tf.maximum(x, 0.2 * x)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7ffpRiA5SRvR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def generator(z):\n","  \"\"\" Generatorul este o retea convolutionala ce face upsample unui vector z\n","       de dimensiune 128. \n","       Are urmatoarele specificatii: FC(4096) -> Reshape(None, 4, 4, 256) ->\n","       Upsample(k5x5, s2x2, c128)-> Upsample(k5x5, s2x2, c64) ->\n","       Upsample(k5x5, s2x2, 1)) -> Sigmoid -> Crop pentru a ajunge in dimensiunea\n","       imaginii originale mnist 28x28x1\n","       \n","       Folositi layerd.fully_connected - pentru Fc, layers_conv2d_transpose pentru\n","       upsample din pachetul contrib din tensorflow\n","  \"\"\"\n","  with tf.variable_scope('generator'):\n","      z = layers.fully_connected(z, num_outputs=4096)\n","      z = tf.reshape(z, [-1, 4, 4, 256])\n","\n","      z = layers.conv2d_transpose(z, num_outputs=128, kernel_size=5, stride=2)\n","      z = layers.conv2d_transpose(z, num_outputs=64, kernel_size=5, stride=2)\n","      z = layers.conv2d_transpose(z, num_outputs=1, kernel_size=5, stride=2,\n","                                  activation_fn=tf.nn.sigmoid)\n","      return z[:, 2:-2, 2:-2, :]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yhzLId63SWLX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def discriminator(x, reuse):\n","  \"\"\" Disciminatorul este o retea convolutionala ce primeste ca input un batch\n","      de imagini.\n","      Reuse=true face sa refoloseasca toate variabilele din scope-ul discriminatorului\n","      Reuse=false creaza pentru prima oara variabilele din reteaua discriminatorului\n","      \n","      Are urmatoarea structura: Conv2d(k5x5, s2x2, c64, LeakyReLU) -> \n","      Conv2d(k5x5, s2x2, c128, LeakyReLU) -> Conv2d(k5x5, s2x2, c256, LeakyReLU)\n","      -> Conv2d(k5x5, s2x2, c256, LeakyReLU) -> Flatten -> FC(1)\n","  \"\"\"\n","  with tf.variable_scope('discriminator', reuse=reuse):\n","      x = layers.conv2d(x, num_outputs=64, kernel_size=5, stride=2,\n","                        activation_fn=leaky_relu)\n","      print(x)\n","      x = layers.conv2d(x, num_outputs=128, kernel_size=5, stride=2,\n","                        activation_fn=leaky_relu)\n","      print(x)\n","      x = layers.conv2d(x, num_outputs=256, kernel_size=5, stride=2,\n","                        activation_fn=leaky_relu)\n","      print(x)\n","\n","      x = layers.flatten(x)\n","      return layers.fully_connected(x, num_outputs=1, activation_fn=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"maWhAbYcSg4O","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["with tf.name_scope('placeholders'):\n","  \"\"\"Generare placoholdere pentru \n","     x_true - un batch de imagini de dimensiunea  28x28x1\n","     z - un batch de noise de dimensiunea 128\n","  \"\"\"\n","  x_true = tf.placeholder(tf.float32, [None, 28, 28, 1])\n","  z = tf.placeholder(tf.float32, [None, 128])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BIg-Xrh8Spo1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":121},"outputId":"618fab3f-714e-428a-dd32-2b958cdcb52d","executionInfo":{"status":"ok","timestamp":1525813087100,"user_tz":-180,"elapsed":692,"user":{"displayName":"Andrei Janca","photoUrl":"//lh4.googleusercontent.com/-WcoatQvgbR4/AAAAAAAAAAI/AAAAAAAAAM0/x-SFTi_dtXM/s50-c-k-no/photo.jpg","userId":"110870304408376504137"}}},"cell_type":"code","source":["\"\"\" \n","Pentru a obtine sample-uri din generator trebuie sa facem o inferenta folosind\n","vectorul de noise z. \n","x_generated - un batch de imagini generate cu o inferenta a zgomotului z\n","\"\"\"\n","x_generated = generator(z)\n","\n","\n","\"\"\"\n","  Pentru a obtine outputul discriminatorului pentru imagini reale si pentru\n","  sample-uri generate de generatoror trebuie sa facem doua inferente prin discriminator: \n","  prima inferenta cu date reale (x_true), iar a doua cu date generate (x_generated)\n","  d_true - outputul discriminatorului pentru date reale\n","  d_generated - outputul generatorului pentru date fake\n","\"\"\"\n","d_true = discriminator(x_true, reuse=False)\n","d_generated = discriminator(x_generated, reuse=True)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Tensor(\"discriminator/Conv/Maximum:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"discriminator/Conv_1/Maximum:0\", shape=(?, 7, 7, 128), dtype=float32)\n","Tensor(\"discriminator/Conv_2/Maximum:0\", shape=(?, 4, 4, 256), dtype=float32)\n","Tensor(\"discriminator_1/Conv/Maximum:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"discriminator_1/Conv_1/Maximum:0\", shape=(?, 7, 7, 128), dtype=float32)\n","Tensor(\"discriminator_1/Conv_2/Maximum:0\", shape=(?, 4, 4, 256), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"iXlGLVTStKgw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"9ac271c7-05fd-4521-e055-b20629b4b349","executionInfo":{"status":"ok","timestamp":1525813091470,"user_tz":-180,"elapsed":936,"user":{"displayName":"Andrei Janca","photoUrl":"//lh4.googleusercontent.com/-WcoatQvgbR4/AAAAAAAAAAI/AAAAAAAAAM0/x-SFTi_dtXM/s50-c-k-no/photo.jpg","userId":"110870304408376504137"}}},"cell_type":"code","source":["print(x_generated)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Tensor(\"generator/strided_slice:0\", shape=(?, 28, 28, 1), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"lSGR2UqwSw7F","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":69},"outputId":"10ac2c7f-d356-4f11-aeeb-bbb4ff7473a2","executionInfo":{"status":"ok","timestamp":1525813094046,"user_tz":-180,"elapsed":1416,"user":{"displayName":"Andrei Janca","photoUrl":"//lh4.googleusercontent.com/-WcoatQvgbR4/AAAAAAAAAAI/AAAAAAAAAM0/x-SFTi_dtXM/s50-c-k-no/photo.jpg","userId":"110870304408376504137"}}},"cell_type":"code","source":["\"\"\"\"\n","  Teoria WGAN din curs spune ca pentru a putea folosi K-R duality pentru a inversa \n","  distanta E-M care da forma loss-ului are nevoie de constrangerea ca functia \n","  determinata de discriminator D(x) trebuie sa fie 1-Lipschitz. \n","\n","  Aceasta constrangere este inpusa prin alegerea unui punct random interpolat intre\n","  datele reale si cele fictive si adaugarea unei functii obiectiv regularizante \n","  care minimizeaza MSE intre norma 2 a gradientul disctriminatorui in raport cu \n","  imaginea interpolata.\n","\"\"\"\n","with tf.name_scope('regularizer'):\n","    epsilon = tf.random_uniform([50, 1, 1, 1], 0.0, 1.0)\n","    x_hat = epsilon * x_true + (1 - epsilon) * x_generated\n","    d_hat = discriminator(x_hat, reuse=True)\n","\n","    gradients = tf.gradients(d_hat, x_hat)[0]\n","    ddx = tf.sqrt(tf.reduce_sum(gradients ** 2, axis=[1, 2]))\n","    d_regularizer = tf.reduce_mean((ddx - 1.0) ** 2)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Tensor(\"regularizer/discriminator/Conv/Maximum:0\", shape=(50, 14, 14, 64), dtype=float32)\n","Tensor(\"regularizer/discriminator/Conv_1/Maximum:0\", shape=(50, 7, 7, 128), dtype=float32)\n","Tensor(\"regularizer/discriminator/Conv_2/Maximum:0\", shape=(50, 4, 4, 256), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"MBuNXBc6S2M0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["with tf.name_scope('loss'):\n","  \"\"\" Crearea functiilor obiectiv pentru generator si pentru discriminator.\n","\n","      Generatorul incearca sa maximizeze valoarea criticului/discriminatorului pentru \n","      datele generate de el (d_generated)\n","      g_loss - este loss-ul generatorului\n","\n","      Discriminatorul incearca sa minimizeze valoarea criticului/discriminatorului\n","      pentru datele generate (d_generated) si incearca sa maximizeze valoarea acestuia\n","      pentru datele reale (d_true).\n","      Aditional mai contribuie si loss-ul de regularizare cu o pandere lambda = 10\n","      d_loss - este lossul discriminatorului\n","  \"\"\"\n","  g_loss = -tf.reduce_mean(d_generated)\n","  d_loss = (-tf.reduce_mean(d_true) + tf.reduce_mean(d_generated) +\n","            10 * d_regularizer)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jg2-4pERS5C1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["with tf.name_scope('optimizer'):\n","  \"\"\"\n","      Generarea optimizatorului - Adam cu un lr=1e-4, beta1=0, beta2=0.9\n","\n","      Generarea variabilelor referitor la care vom optimiza: \n","      pentru generator le putem accesa folosing colectia variabilelor globale cu scope-ul\n","      cu care au fost create, idem si pentru discriminator.\n","      g_train - contine tensorul de optimizare\n","\n","      Cu aceste variabile putem optimiza fiecare loss in parte raportat la var lui.\n","      Generator - minimizam loss-ul g_loss raportat la variabilele generatorului\n","      Discriminator - minimizam loss-ul d_loss raportat la variabilele discriminatorului\n","      d_train - contine tensorul de optimizare\n","  \"\"\"\n","\n","  optimizer = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0, beta2=0.9)\n","  g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n","  g_train = optimizer.minimize(g_loss, var_list=g_vars)\n","  d_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n","  d_train = optimizer.minimize(d_loss, var_list=d_vars)\n","\n","  d_summary = tf.summary.merge([tf.summary.scalar('d_loss', d_loss)])\n","  g_summary = tf.summary.merge([tf.summary.scalar('g_loss', g_loss)])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LfUHILaC_q8u","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Generare global_step si o operatie ce il incrementeaza\n","global_step = tf.Variable(0, dtype=tf.int32, name='global_step', trainable=False)\n","increment_global_step = global_step.assign_add(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C-KAwRehlR-K","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Definire saver, generare de foldere pentru modele, summaries\n","saver = tf.train.Saver()\n","if resume:\n","  model_path = os.path.join(\"models\")\n","  summary_path = os.path.join(\"summaries\")\n","  if os.path.exists(model_path):\n","    tf.gfile.DeleteRecursively(model_path)\n","  if os.path.exists(summary_path):\n","    tf.gfile.DeleteRecursively(summary_path)\n","  tf.gfile.MakeDirs(model_path)\n","  tf.gfile.MakeDirs(summary_path)\n","  \n","  summary_writer = tf.summary.FileWriter(summary_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1Fx3mLxMS_6d","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":237},"outputId":"9051f978-79c9-4548-ec8c-06be54296328","executionInfo":{"status":"error","timestamp":1525813110216,"user_tz":-180,"elapsed":1542,"user":{"displayName":"Andrei Janca","photoUrl":"//lh4.googleusercontent.com/-WcoatQvgbR4/AAAAAAAAAAI/AAAAAAAAAM0/x-SFTi_dtXM/s50-c-k-no/photo.jpg","userId":"110870304408376504137"}}},"cell_type":"code","source":["#Initializare sau resume pentru model\n","if resume:\n","  session.run(tf.global_variables_initializer())\n","  ckpt = tf.train.get_checkpoint_state(model_path)\n","  tf.logging.info(\"Loading Model from {}\".format(ckpt.model_checkpoint_path))\n","  saver.restore(session, ckpt.model_checkpoint_path)\n","  session.run(tf.local_variables_initializer())\n","else:\n","  session.run([tf.global_variables_initializer(), tf.local_variables_initializer()])"],"execution_count":15,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-e498448a13d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Model from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'model_checkpoint_path'"]}]},{"metadata":{"id":"1afaI5GQTGzT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":489},"outputId":"54bb96eb-b72c-4653-858f-5216cddff65d","executionInfo":{"status":"ok","timestamp":1525813152104,"user_tz":-180,"elapsed":1476,"user":{"displayName":"Andrei Janca","photoUrl":"//lh4.googleusercontent.com/-WcoatQvgbR4/AAAAAAAAAAI/AAAAAAAAAM0/x-SFTi_dtXM/s50-c-k-no/photo.jpg","userId":"110870304408376504137"}}},"cell_type":"code","source":["#Citirea datasetului \n","mnist = input_data.read_data_sets('MNIST_data')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-16-b8fd4c8de6c9>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use urllib or similar directly.\n","Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting MNIST_data/train-images-idx3-ubyte.gz\n","Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting MNIST_data/train-labels-idx1-ubyte.gz\n","Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n","Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n","Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n","Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"],"name":"stdout"}]},{"metadata":{"id":"BjD6zSXmTIqj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":2900},"outputId":"b6b983a4-27bf-46c3-819e-9f2d1cb3fee1"},"cell_type":"code","source":["i = session.run(global_step)\n","\n","#Pentru un numar iteratii:\n","\n","while i <= 20000:\n","  # extragem un batch de date reale\n","  batch = mnist.train.next_batch(50)\n","  # reshape la dimensiunile 28x28x1\n","  images = batch[0].reshape([-1, 28, 28, 1])\n","  # samplam un batch de zgomot\n","  z_train = np.random.randn(50, 128)\n","\n","  # executam gradient descent pe generator feed-uind un batch de zgomot\n","  session.run(g_train, feed_dict={z: z_train})\n","\n","  # optimizam in loop discriminatorul pentru un numar de iteratii feed-urind\n","  # un batch de imagini reale si un batch de zgomot pentru cele generate\n","  for j in range(5):\n","    session.run(d_train, feed_dict={x_true: images, z: z_train})\n","\n","  if i % 100 == 0:\n","    g_summ = session.run(g_summary, feed_dict={z: z_train})\n","    d_summ = session.run(d_summary, feed_dict={x_true: images, z: z_train})\n","    summary_writer.add_summary(d_summ, i)\n","    summary_writer.add_summary(g_summ, i)\n","\n","  # la fiecare 100 de pasi afisam o imagine generata\n","  if i % 100 == 0:\n","    print('iter={}/20000'.format(i))\n","    # samplam 1 vector de zgomot\n","    z_validate = np.random.randn(1, 128)\n","    # rulam tensorul ce contine imaginea generata feeduing zgomotul si afisam\n","    # matplotlib imaginea\n","    generated = x_generated.eval(feed_dict={z: z_validate}).squeeze()\n","\n","    # Image.fromarray(generated.astype(np.uint8)).save(os.path.join(images_path, \"image_{}.png\".format(i)))\n","    plt.figure('results')\n","    plt.imshow(generated, clim=[0, 1], cmap='bone')\n","#     plt.pause(0.001)\n","#     display.clear_output(wait=True)\n","#     display.display(plt.gcf())\n","#     time.sleep(0.001)\n","\n","    saver.save(session, model_path + '/model-' + str(i) + '.cptk',\n","               global_step=global_step)\n","    tf.logging.info(\"Saved Model at {}\".format(model_path + '/model-' + str(i) + '.cptk'))\n","\n","\n","\n","  session.run(increment_global_step)\n","  i += 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["iter=0/20000\n","INFO:tensorflow:Saved Model at models/model-0.cptk\n","iter=100/20000\n","INFO:tensorflow:Saved Model at models/model-100.cptk\n","iter=200/20000\n","INFO:tensorflow:Saved Model at models/model-200.cptk\n","iter=300/20000\n","INFO:tensorflow:Saved Model at models/model-300.cptk\n","iter=400/20000\n","INFO:tensorflow:Saved Model at models/model-400.cptk\n","iter=500/20000\n","INFO:tensorflow:Saved Model at models/model-500.cptk\n","iter=600/20000\n","INFO:tensorflow:Saved Model at models/model-600.cptk\n","iter=700/20000\n","INFO:tensorflow:Saved Model at models/model-700.cptk\n","iter=800/20000\n","INFO:tensorflow:Saved Model at models/model-800.cptk\n","iter=900/20000\n","INFO:tensorflow:Saved Model at models/model-900.cptk\n","iter=1000/20000\n","INFO:tensorflow:Saved Model at models/model-1000.cptk\n","iter=1100/20000\n","INFO:tensorflow:Saved Model at models/model-1100.cptk\n","iter=1200/20000\n","INFO:tensorflow:Saved Model at models/model-1200.cptk\n","iter=1300/20000\n","INFO:tensorflow:Saved Model at models/model-1300.cptk\n","iter=1400/20000\n","INFO:tensorflow:Saved Model at models/model-1400.cptk\n","iter=1500/20000\n","INFO:tensorflow:Saved Model at models/model-1500.cptk\n","iter=1600/20000\n","INFO:tensorflow:Saved Model at models/model-1600.cptk\n","iter=1700/20000\n","INFO:tensorflow:Saved Model at models/model-1700.cptk\n","iter=1800/20000\n","INFO:tensorflow:Saved Model at models/model-1800.cptk\n","iter=1900/20000\n","INFO:tensorflow:Saved Model at models/model-1900.cptk\n","iter=2000/20000\n","INFO:tensorflow:Saved Model at models/model-2000.cptk\n","iter=2100/20000\n","INFO:tensorflow:Saved Model at models/model-2100.cptk\n","iter=2200/20000\n","INFO:tensorflow:Saved Model at models/model-2200.cptk\n","iter=2300/20000\n","INFO:tensorflow:Saved Model at models/model-2300.cptk\n","iter=2400/20000\n","INFO:tensorflow:Saved Model at models/model-2400.cptk\n","iter=2500/20000\n","INFO:tensorflow:Saved Model at models/model-2500.cptk\n","iter=2600/20000\n","INFO:tensorflow:Saved Model at models/model-2600.cptk\n","iter=2700/20000\n","INFO:tensorflow:Saved Model at models/model-2700.cptk\n","iter=2800/20000\n","INFO:tensorflow:Saved Model at models/model-2800.cptk\n","iter=2900/20000\n","INFO:tensorflow:Saved Model at models/model-2900.cptk\n","iter=3000/20000\n","INFO:tensorflow:Saved Model at models/model-3000.cptk\n","iter=3100/20000\n","INFO:tensorflow:Saved Model at models/model-3100.cptk\n","iter=3200/20000\n","INFO:tensorflow:Saved Model at models/model-3200.cptk\n","iter=3300/20000\n","INFO:tensorflow:Saved Model at models/model-3300.cptk\n","iter=3400/20000\n","INFO:tensorflow:Saved Model at models/model-3400.cptk\n","iter=3500/20000\n","INFO:tensorflow:Saved Model at models/model-3500.cptk\n","iter=3600/20000\n","INFO:tensorflow:Saved Model at models/model-3600.cptk\n","iter=3700/20000\n","INFO:tensorflow:Saved Model at models/model-3700.cptk\n","iter=3800/20000\n","INFO:tensorflow:Saved Model at models/model-3800.cptk\n","iter=3900/20000\n","INFO:tensorflow:Saved Model at models/model-3900.cptk\n","iter=4000/20000\n","INFO:tensorflow:Saved Model at models/model-4000.cptk\n","iter=4100/20000\n","INFO:tensorflow:Saved Model at models/model-4100.cptk\n","iter=4200/20000\n","INFO:tensorflow:Saved Model at models/model-4200.cptk\n","iter=4300/20000\n","INFO:tensorflow:Saved Model at models/model-4300.cptk\n","iter=4400/20000\n","INFO:tensorflow:Saved Model at models/model-4400.cptk\n","iter=4500/20000\n","INFO:tensorflow:Saved Model at models/model-4500.cptk\n","iter=4600/20000\n","INFO:tensorflow:Saved Model at models/model-4600.cptk\n","iter=4700/20000\n","INFO:tensorflow:Saved Model at models/model-4700.cptk\n","iter=4800/20000\n","INFO:tensorflow:Saved Model at models/model-4800.cptk\n","iter=4900/20000\n","INFO:tensorflow:Saved Model at models/model-4900.cptk\n","iter=5000/20000\n","INFO:tensorflow:Saved Model at models/model-5000.cptk\n","iter=5100/20000\n","INFO:tensorflow:Saved Model at models/model-5100.cptk\n"],"name":"stdout"},{"output_type":"stream","text":["iter=5200/20000\n","INFO:tensorflow:Saved Model at models/model-5200.cptk\n","iter=5300/20000\n","INFO:tensorflow:Saved Model at models/model-5300.cptk\n","iter=5400/20000\n","INFO:tensorflow:Saved Model at models/model-5400.cptk\n","iter=5500/20000\n","INFO:tensorflow:Saved Model at models/model-5500.cptk\n","iter=5600/20000\n","INFO:tensorflow:Saved Model at models/model-5600.cptk\n","iter=5700/20000\n","INFO:tensorflow:Saved Model at models/model-5700.cptk\n","iter=5800/20000\n","INFO:tensorflow:Saved Model at models/model-5800.cptk\n","iter=5900/20000\n","INFO:tensorflow:Saved Model at models/model-5900.cptk\n","iter=6000/20000\n","INFO:tensorflow:Saved Model at models/model-6000.cptk\n","iter=6100/20000\n","INFO:tensorflow:Saved Model at models/model-6100.cptk\n","iter=6200/20000\n","INFO:tensorflow:Saved Model at models/model-6200.cptk\n","iter=6300/20000\n","INFO:tensorflow:Saved Model at models/model-6300.cptk\n","iter=6400/20000\n","INFO:tensorflow:Saved Model at models/model-6400.cptk\n","iter=6500/20000\n","INFO:tensorflow:Saved Model at models/model-6500.cptk\n","iter=6600/20000\n","INFO:tensorflow:Saved Model at models/model-6600.cptk\n","iter=6700/20000\n","INFO:tensorflow:Saved Model at models/model-6700.cptk\n","iter=6800/20000\n","INFO:tensorflow:Saved Model at models/model-6800.cptk\n","iter=6900/20000\n","INFO:tensorflow:Saved Model at models/model-6900.cptk\n","iter=7000/20000\n","INFO:tensorflow:Saved Model at models/model-7000.cptk\n","iter=7100/20000\n","INFO:tensorflow:Saved Model at models/model-7100.cptk\n","iter=7200/20000\n","INFO:tensorflow:Saved Model at models/model-7200.cptk\n","iter=7300/20000\n","INFO:tensorflow:Saved Model at models/model-7300.cptk\n","iter=7400/20000\n","INFO:tensorflow:Saved Model at models/model-7400.cptk\n","iter=7500/20000\n","INFO:tensorflow:Saved Model at models/model-7500.cptk\n","iter=7600/20000\n","INFO:tensorflow:Saved Model at models/model-7600.cptk\n","iter=7700/20000\n","INFO:tensorflow:Saved Model at models/model-7700.cptk\n","iter=7800/20000\n","INFO:tensorflow:Saved Model at models/model-7800.cptk\n","iter=7900/20000\n","INFO:tensorflow:Saved Model at models/model-7900.cptk\n","iter=8000/20000\n","INFO:tensorflow:Saved Model at models/model-8000.cptk\n","iter=8100/20000\n","INFO:tensorflow:Saved Model at models/model-8100.cptk\n","iter=8200/20000\n","INFO:tensorflow:Saved Model at models/model-8200.cptk\n"],"name":"stdout"}]}]}