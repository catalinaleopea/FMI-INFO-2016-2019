{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Solutie_Leopea_Catalina_lab05.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"wpHziT7hIcg7","colab_type":"text"},"cell_type":"markdown","source":["## Laborator 4 Versiune Imbunatatita\n","\n","## Obiective\n","\n","* familiaziraze cu tensorflow slim.\n","* modificarea unei retele neurale in slim\n","* vizualizare date input\n","* vizualizare activari\n","* tf.metrics\n"]},{"metadata":{"id":"dXu-4AmXLcjn","colab_type":"text"},"cell_type":"markdown","source":["## Pasul 0. Upload dependinte Python#\n","\n","* cifar10.py\n","* download.py\n","* dataset.py\n","* cache.py\n"]},{"metadata":{"id":"srTcnQt9uBGQ","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","from google.colab import files\n","uploaded = files.upload()\n","print(\"OK\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RJrs-lx_MPjU","colab_type":"code","colab":{}},"cell_type":"code","source":["# verificam ca totul este ok\n","!ls ."],"execution_count":0,"outputs":[]},{"metadata":{"id":"T5ja2vEKMGyp","colab_type":"text"},"cell_type":"markdown","source":["## Pasul 1. Incarcarea dataset-ului"]},{"metadata":{"id":"yynlRpNIujw_","colab_type":"code","colab":{}},"cell_type":"code","source":["import cifar10\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ru7DT110vXrI","colab_type":"code","colab":{}},"cell_type":"code","source":["cifar10.maybe_download_and_extract()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O84le8Amu_tl","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls data/CIFAR-10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nXdp3l8CMdHP","colab_type":"text"},"cell_type":"markdown","source":["## Pasul 2. Inspecatarea dataset-ului. Histograma + Imagini sample"]},{"metadata":{"id":"RSPRpBDSxeUk","colab_type":"code","colab":{}},"cell_type":"code","source":["class_names = cifar10.load_class_names()\n","class_names"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TjodQ6lyxhFG","colab_type":"code","colab":{}},"cell_type":"code","source":["images_train, cls_train, labels_train = cifar10.load_training_data()\n","images_test, cls_test, labels_test = cifar10.load_test_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kuJpQuqqxjPe","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","print(images_train.shape)\n","print(images_test.shape)\n","\n","# one hot encodings\n","print(labels_train.shape)\n","print(labels_test.shape)\n","\n","# class labels\n","print(cls_train.shape)\n","print(cls_test.shape)\n","\n","\n","print(\"Train count {}\".format(images_train.shape[0]))\n","print(\"Test count {}\".format(images_test.shape[0]))\n","\n","cls_ids = np.unique(cls_train)\n","print(\"Class labels {}.\".format(cls_ids))\n","\n","n_classes = len(cls_ids)\n","print(\"Num classes {}\".format(n_classes))\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2I1HjPlIyBbS","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","import time\n","from datetime import timedelta\n","import math\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yxnn8abZNqlm","colab_type":"code","colab":{}},"cell_type":"code","source":["# check data statistics\n","\n","def get_stats(labels):\n","    stats = np.zeros(n_classes)\n","    for e in labels:\n","        stats[e] += 1\n","    return stats\n","  \n","# bar_width = 0.\n","def plot_stats(stats, title):\n","    plt.figure()\n","    x = range(n_classes)\n","    plt.title(title)\n","    plt.bar(x, stats)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bGkebuIixo8h","colab_type":"code","colab":{}},"cell_type":"code","source":["y_train = cls_train\n","y_test = cls_test\n","\n","X_train = images_train\n","X_test = images_test\n","\n","train_stats = get_stats(y_train)\n","test_stats = get_stats(y_test)\n","\n","plt.figure()\n","plot_stats(train_stats, \"Training samples/class\")\n","plot_stats(test_stats, \"Testing samples/class\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KuT2ljMZx6SK","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_images(images, cls_true, cls_pred=None, smooth=True):\n","\n","    assert len(images) == len(cls_true) == 9\n","\n","    # Create figure with sub-plots.\n","    fig, axes = plt.subplots(3, 3)\n","\n","    # Adjust vertical spacing if we need to print ensemble and best-net.\n","    if cls_pred is None:\n","        hspace = 0.3\n","    else:\n","        hspace = 0.6\n","    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n","\n","    for i, ax in enumerate(axes.flat):\n","        # Interpolation type.\n","        if smooth:\n","            interpolation = 'spline16'\n","        else:\n","            interpolation = 'nearest'\n","\n","        # Plot image.\n","        ax.imshow(images[i, :, :, :],\n","                  interpolation=interpolation)\n","            \n","        # Name of the true class.\n","        cls_true_name = class_names[cls_true[i]]\n","\n","        # Show true and predicted classes.\n","        if cls_pred is None:\n","            xlabel = \"True: {0}\".format(cls_true_name)\n","        else:\n","            # Name of the predicted class.\n","            cls_pred_name = class_names[cls_pred[i]]\n","\n","            xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n","\n","        # Show the classes as the label on the x-axis.\n","        ax.set_xlabel(xlabel)\n","        \n","        # Remove ticks from the plot.\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","    \n","    # Ensure the plot is shown correctly with multiple plots\n","    # in a single Notebook cell.\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6rKo4yGSx7jJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get the first images from the test-set.\n","images = images_test[0:9]\n","\n","# Get the true classes for those images.\n","cls_true = cls_test[0:9]\n","\n","# Plot the images and labels using our helper-function above.\n","plot_images(images=images, cls_true=cls_true, smooth=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7t8_rfHMQD_g","colab_type":"text"},"cell_type":"markdown","source":["### Vizualizati dataset-ul cu sample-uri din fiecare clasa."]},{"metadata":{"id":"EP1UApvnOTBe","colab_type":"code","colab":{}},"cell_type":"code","source":["num_samples = 10\n","\n","image_shape = images_train.shape[1:]\n","\n","img_height, img_width = image_shape[0], image_shape[1]\n","\n","print(\"hxw {}x{}\".format(img_height, img_width))\n","\n","def draw_samples(X, y, y_target, num_cl, colormap = None):\n","    c_ids = np.where(y == y_target)\n","    selected_idx = random.sample(c_ids[0].tolist(), num_samples)\n","    for i, idx in enumerate(selected_idx):\n","        plt.subplot(num_cl, num_samples, (num_samples * y_target) + i + 1)\n","        plt.axis('off')\n","        plt.imshow(X[idx], colormap)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QRpJJ8A9OXQ1","colab_type":"code","colab":{}},"cell_type":"code","source":["import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# index = random.randint(0, len(X_train))\n","# image = X_train[index].squeeze()\n","\n","\n","\n","plt.figure(figsize=(num_samples, img_width))\n","for c in range(n_classes):\n","    draw_samples(X_train, y_train, c, n_classes)\n","# \n","# plt.imshow(image)\n","# plt.axis('off')\n","# print(y_train[index])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ptJWMdiOPNhp","colab_type":"text"},"cell_type":"markdown","source":["## Pasul 3. Definirea modelului in TensorFlow\n","\n","### *Nota* Vom folosi tensoflow slim pentru a ne face viata mai usoara."]},{"metadata":{"id":"jrCKThG-182E","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow.contrib.slim as slim\n","\n","trunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FmjeHX4yP5Sa","colab_type":"text"},"cell_type":"markdown","source":["\n","## Definirea unui argument scope. \n","\n","Un argument scope furnizeaza parametri default pentru operatiile din slim.\n","Pentru a consulta parametri default, ne putem uita direct in codul sursa:\n","\n","https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L917"]},{"metadata":{"id":"xAVxGR2EhDKV","colab_type":"code","colab":{}},"cell_type":"code","source":["def cifarnet_arg_scope_bnorm(weight_decay=0.004, is_training=True):\n","  \"\"\"Defines the batch norm cifarnet argument scope.\n","\n","  Args:\n","    weight_decay: The weight decay to use for regularizing the model.\n","\n","  Returns:\n","       An `arg_scope` to use for the cifarnet model.\n","  \"\"\"\n","  \n","  batch_norm_params = {\n","      'is_training': is_training,\n","      'center': True,\n","      'scale': True,\n","      'decay': 0.997,\n","      'epsilon': 0.001,\n","  }\n","  \n","  with slim.arg_scope(\n","      [slim.conv2d],\n","      weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n","      activation_fn=tf.nn.relu6,\n","      normalizer_fn=slim.batch_norm):\n","    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n","      with slim.arg_scope(\n","          [slim.fully_connected],\n","          biases_initializer=tf.constant_initializer(0.1),\n","          weights_initializer=tf.contrib.layers.xavier_initializer(),\n","          weights_regularizer=slim.l2_regularizer(weight_decay),\n","\n","          activation_fn=tf.nn.relu) as sc:\n","         \n","          return sc\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8-TnZPwpSVlv","colab_type":"text"},"cell_type":"markdown","source":["## CifarNet Inception"]},{"metadata":{"id":"Otg1lZmIxVtV","colab_type":"code","colab":{}},"cell_type":"code","source":["def inception_module(net, maps, scope=None, reuse=None):\n","  conv_1x1_map = maps[0]   # număr feature maps branch 1\n","  reduce_3x3_map = maps[1] # număr feature maps reduction branch 2\n","  reduce_5x5_map = maps[2] # număr feature maps reduction branch 3\n","  conv_3x3_map = maps[3]   # număr feature maps branch 2\n","  conv_5x5_map = maps[4]   # număr feature maps branch 3\n","  conv_1x1_4_map = maps[5] # număr feature maps branch 4\n","  \n","  ###################################\n"," \n","  with tf.variable_scope(scope, \"InceptionBlock\", [net], reuse=reuse):\n","    with tf.variable_scope('Branch_1'):\n","      conv1_1x1 = slim.conv2d(net, conv_1x1_map, [1, 1], \n","                               padding = 'SAME', scope = 'conv1_1x1')\n","     \n","    with tf.variable_scope('Branch_2'):\n","      conv2_1x1 = slim.conv2d(net, reduce_3x3_map, [1, 1], \n","                               padding = 'SAME', scope = 'conv2_1x1')\n","      conv2_3x3 = slim.conv2d(conv2_1x1, conv_3x3_map, [3, 3], \n","                               padding = 'SAME', scope = 'conv2_3x3')\n","      \n","    with tf.variable_scope('Branch_3'):\n","      conv3_1x1 = slim.conv2d(net, reduce_5x5_map, [1, 1], \n","                               padding = 'SAME', scope = 'conv3_1x1')\n","      conv3_5x5 = slim.conv2d(conv3_1x1, conv_5x5_map, [5, 5], \n","                               padding = 'SAME', scope = 'conv3_5x5')\n","      \n","    with tf.variable_scope('Branch_4'):\n","      max_pool_3x3 = slim.max_pool2d(net, [3,3], stride = 1, padding = 'SAME')\n","      conv4_1x1 = slim.conv2d(max_pool_3x3, conv_1x1_4_map, [1, 1], \n","                               padding = 'SAME', scope = 'conv4_1x1')\n","    \n","    net = tf.concat([conv1_1x1, conv2_3x3, conv3_5x5, conv4_1x1],axis = 3, \n","                    name = 'Inception')\n","    \n","  ###################################\n","  \n","  \n","  return net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oOKURdnR1HKy","colab_type":"code","colab":{}},"cell_type":"code","source":["trunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n","\n","def cifarnet_bn(images, num_classes=10, is_training=False,\n","             dropout_keep_prob=0.5,\n","             prediction_fn=slim.softmax,\n","             scope='CifarNet'):\n"," \n","  end_point = {}\n","\n","  with tf.variable_scope(scope, 'CifarNet', [images]):\n"," \n","    net = slim.conv2d(images, 32, [3,3], \n","                      padding='SAME', scope='stem_conv1')\n","\n","    net = slim.conv2d(net, 96, [3,3], \n","                      padding='SAME', scope='stem_conv2')\n","\n","    net = inception_module(net,[32, 96, 16, 64, 16, 16]) \n","    end_point['inception1'] = net\n","\n","\n","    net = inception_module(net,[64, 128, 32, 96, 48, 32]) \n","    end_point['inception2'] = net\n","\n","\n","    net = slim.max_pool2d(net,[3, 3],stride = 2, padding = 'SAME', scope = 'maxpool')\n","    end_point['maxpool'] = net\n","\n","\n","    net = inception_module(net,[96, 96, 16, 104, 24, 32]) \n","    end_point['inception3'] = net\n","\n","\n","    net = slim.avg_pool2d(net,[5, 5],stride = 3, padding = 'SAME', scope = 'avgpool')\n","    end_point['avgpool'] = net\n","\n","\n","    net = slim.flatten(net)\n","    end_point['flatten'] = net\n","\n","    logits = slim.fully_connected(net, num_classes,\n","                                 biases_initializer = tf.zeros_initializer(),\n","                                 weights_initializer = tf.contrib.layers.xavier_initializer(),\n","                                 weights_regularizer = None,\n","                                 activation_fn = None,\n","                                 scope = 'logits')\n","    end_point['Logits'] = net\n","\n","    end_point['Predictions'] = prediction_fn(logits, scope = 'Predictions')\n","\n","  return logits, end_point"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r4Be5RPN3vji","colab_type":"code","colab":{}},"cell_type":"code","source":["# parametri de training si input\n","batch_size = 32\n","height = 32\n","width = 32\n","channels = 3\n","num_classes = 10\n","initial_learning_rate = 1e-4"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4VvaEQ1xJnBx","colab_type":"text"},"cell_type":"markdown","source":["### Adaugarea conexiunilor de intrare. tf.placeholders"]},{"metadata":{"id":"K_UnNVVv3qCT","colab_type":"code","colab":{}},"cell_type":"code","source":["    def add_preprocessing(image_input, is_training):\n","      \n","        def _process_image(augment_level, image):\n","            # Because these operations are not commutative, consider randomizing\n","            # randomize the order their operation.\n","            if augment_level > 0:\n","                image = tf.image.random_brightness(image, max_delta=30)\n","                image = tf.image.random_contrast(image, lower=0.75, upper=1.25)\n","            if augment_level > 1:\n","                image = tf.image.random_saturation(image, lower=0.5, upper=1.6)\n","                image = tf.image.random_hue(image, max_delta=0.15)\n","            image = tf.minimum(image, 255.0)\n","            image = tf.maximum(image, 0)\n","            return image\n","\n","        def _preprocess_train(input_tensor):\n","            input_tensor = tf.image.random_flip_left_right(input_tensor)\n","            input_tensor = tf.subtract(input_tensor, 0.5)\n","            input_tensor = tf.multiply(input_tensor, 2.0)\n","\n","            return input_tensor\n","          \n","        def _preprocess_test(input_tensor):\n","            input_tensor = tf.subtract(input_tensor, 0.5)\n","            input_tensor = tf.multiply(input_tensor, 2.0)\n","            \n","            return input_tensor\n","          \n","        preprocessed_input = tf.map_fn(lambda img:\n","                                 tf.cond(\n","                                   tf.equal(\n","                                        is_training,\n","                                     tf.constant(True)),\n","                                    lambda: _preprocess_train(img),\n","                                    lambda: _preprocess_test(img)), image_input)\n","\n","        return preprocessed_input\n","          \n","        \n","    def add_placeholders():\n","        \n","        # image batch input\n","        image_input = tf.placeholder(\n","            tf.float32, [None, height, width, 3],\n","            name='image_input'\n","        )\n","\n","        label_input = tf.placeholder(\n","            tf.int64, [None],\n","            name='label_input'\n","        )\n","        \n","        is_training = tf.placeholder(tf.bool, name='is_training')\n","        \n","        learning_rate = tf.placeholder(tf.float32, shape=[])\n","        \n","        return image_input, label_input, is_training, learning_rate\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1XHBdKzBx28d","colab_type":"text"},"cell_type":"markdown","source":["### Nota\n","Pentru augmentare adaugati o operatie tensorflow in input:\n","\n","```\n","\n","if is_training:\n","    scaled_input_tensor = tf.scalar_mul((1.0 / 255), _process_image(1, image_input))\n","else:\n","    scaled_input_tensor = tf.scalar_mul((1.0 / 255), self.image_input)\n","\n","scaled_input_tensor = tf.subtract(scaled_input_tensor, 0.5)\n","self.scaled_input_tensor = tf.multiply(scaled_input_tensor, 2.0)\n","\n","```\n","\n","Mai sus se opereaza folosind `_process_image` augmenatere de brgithnes si saturation.\n","Inainte, input-ul este normalizat.\n","\n","Normalizarea se efectueaza atat la training cat si la testing.\n","Augmentarea se efectueaza doar la testing si are rol in generelizare.\n","\n","Puteti folosi si horizontal flipping `tf.image.random_flip_left_right`\n","\n","https://www.tensorflow.org/api_docs/python/tf/image/random_flip_left_right\n"]},{"metadata":{"id":"x_q1yGSB7s1A","colab_type":"code","colab":{}},"cell_type":"code","source":["images_train.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"21vqUW0c_X2g","colab_type":"code","colab":{}},"cell_type":"code","source":["labels_train.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TT3OpO89_5UW","colab_type":"code","colab":{}},"cell_type":"code","source":["np.unique(cls_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yDMGR4tDJ2Dw","colab_type":"text"},"cell_type":"markdown","source":["### Functie helper pentru a incarca un minibatch random la training"]},{"metadata":{"id":"LMtvX9xwASl8","colab_type":"code","colab":{}},"cell_type":"code","source":["def random_batch(img, labels, bsize=32):\n","    # Number of images in the training-set.\n","    num_images = len(img)\n","    #     print(num_images)\n","\n","    # Create a random index.\n","    idx = np.random.choice(num_images,\n","                           size=bsize,\n","                           replace=False)\n","\n","    # Use the random index to select random images and labels.\n","    x_batch = img[idx, :, :, :]\n","    y_batch = labels[idx]\n","\n","    return x_batch, y_batch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kicRVSn9AWzZ","colab_type":"code","colab":{}},"cell_type":"code","source":["x, y = random_batch(X_train, y_train)\n","\n","y.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C-sZgA4aKSIU","colab_type":"text"},"cell_type":"markdown","source":["### Functie helper pentru obtinirea unui batch la test. Nu facem shuffle la test"]},{"metadata":{"id":"H68HVlnossrZ","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_batch(imgs, labels, step, bsize=32):\n","  offset = (step * batch_size) % (labels.shape[0] - batch_size)\n","#   print(offset)\n","  batch_imgs = imgs[offset:(offset + bsize), :, :, :]\n","  batch_labels = labels[offset:(offset + bsize)]\n","  \n","  return batch_imgs, batch_labels\n","  \n","batch_imgs, batch_labels = get_batch(images_test, cls_test, 2)\n","  \n","print(batch_imgs.shape)\n","print(batch_labels.shape)\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"1RPubRTLKc0k","colab_type":"text"},"cell_type":"markdown","source":["### Functie pentru evaluare acuratete"]},{"metadata":{"id":"YA_L36h0ay7X","colab_type":"code","colab":{}},"cell_type":"code","source":["#evaluate model\n","\n","\n","def evaluate():\n","  bsize = 32\n","  total_examples = cls_test.shape[0]  \n","  iters = int(total_examples/bsize)\n","  \n","  acc = []\n","  losses = []\n","    \n","  for i in range(iters):\n","    \n","    x, y = get_batch(images_test, cls_test, i, bsize)\n","\n","    feed_dict = {\n","\n","        image_input: x,\n","        label_input: y,\n","        is_training: False\n","\n","    }\n","\n","    testAcc, testLoss = sess.run([accuracy, loss], feed_dict=feed_dict)\n","    acc.append(testAcc)\n","    losses.append(testLoss)\n","    \n","  meanAcc = np.mean(np.asarray(acc))\n","  meanLoss = np.mean(np.asarray(losses))\n","  \n","  return meanAcc, meanLoss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t094SL0wKuuT","colab_type":"code","colab":{}},"cell_type":"code","source":["trainingAccuracyList = []\n","trainingLossList = []\n","testAccuracyList = []\n","testLossList = []\n","learningRateList = []"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-UUqX-AVCeWF","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.reset_default_graph()\n","\n","g = tf.Graph().as_default()\n","image_input, label_input, is_training, learning_rate = add_placeholders()\n","preprocessed_image_input = add_preprocessing(image_input, is_training)\n","\n","\n","arg_scope = cifarnet_arg_scope_bnorm(is_training=is_training)\n","with slim.arg_scope(arg_scope):\n","  logits, end_points = cifarnet_bn(preprocessed_image_input, is_training=is_training)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ub2tDVS-KAJo","colab_type":"text"},"cell_type":"markdown","source":["### Definirea pasilor de antrenare"]},{"metadata":{"id":"JAMeODSj_w4P","colab_type":"code","colab":{}},"cell_type":"code","source":["initial_learning_rate = 1e-3\n","num_steps = int(50000)\n","num_examples = images_train.shape[0]\n","iters = num_examples / batch_size\n","learning_rate_step = 10000\n","learning_rate_decay = 0.5\n","\n","\n","\n","loss = tf.reduce_mean(\n","  tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label_input, logits=logits))\n","\n","# accuracy of the trained model, between 0 (worst) and 1 (best)\n","predictions = end_points['Predictions']\n","\n","correct_prediction = tf.equal(tf.argmax(predictions, 1), label_input)\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","\n","\n","\n","\n","# Optimizer.\n","# optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n","\n","# for batch norm training. Note: we should use slim.train_op\n","\n","update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","with tf.control_dependencies(update_ops):\n","  # Ensures that we execute the update_ops before performing the train_step\n","#   optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n","  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n","  \n","  \n","  \n","  \n","init = tf.global_variables_initializer()\n","\n","\n","sess = tf.Session()\n","# actually initialize our variables\n","sess.run(init)\n","\n","\n","  \n","running_lr = initial_learning_rate\n","\n","print(\"Starting optimization\")\n","print(\"Batch size {}\".format(batch_size))\n","print(\"Initial LR {}. LR stepdown itnerval {}. LR deacy factor {}\".format(running_lr, learning_rate_step, learning_rate_decay))\n","\n","for i in range(num_steps):\n","  x, y = random_batch(X_train, y_train, bsize=batch_size)\n","\n","  feed_dict = {\n","\n","      image_input: x,\n","      label_input: y,\n","      is_training: True,\n","      learning_rate : running_lr\n","\n","  }\n","\n","  if i % 200 == 0:\n","      _, trainAcc, trainLoss = sess.run([optimizer, accuracy, loss], feed_dict=feed_dict)\n","      \n","      testAcc, testLoss = evaluate()\n","      \n","      print(\"Train \" + str(i) + \": accuracy:\" + str(trainAcc) + \" loss: \" + str(trainLoss))\n","      print(\"Test \" + str(i) + \": accuracy:\" + str(testAcc) + \" loss: \" + str(testLoss))\n","      \n","      trainingAccuracyList.append(trainAcc)\n","      trainingLossList.append(trainLoss)\n","      testAccuracyList.append(testAcc)\n","      testLossList.append(testLoss)\n","      learningRateList.append(running_lr)\n","     \n","  else:\n","      sess.run([optimizer], feed_dict=feed_dict)\n","  \n","  \n","  if  i > 0 and i % learning_rate_step == 0:\n","      print(\"Learning reate step down. Old {}. New {}\".format(running_lr, running_lr * learning_rate_decay))\n","      running_lr = running_lr * learning_rate_decay\n","      \n","          \n","    \n","\n","\n","\n","\n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"rsU44I5hNZSn","colab_type":"text"},"cell_type":"markdown","source":["### Plotting"]},{"metadata":{"id":"UM0RCLM8J-oG","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure(figsize=(20,8))\n","\n","# Plot Accuracy\n","plt.subplot(1,2,1);\n","plt.plot(trainingAccuracyList, label=\"Train Acc\");\n","plt.plot(testAccuracyList, label=\"Test Acc\");\n","plt.title(\"Accuracy\");\n","plt.legend();\n","\n","# Plot Loss\n","plt.subplot(1,2,2);\n","plt.plot(trainingLossList, label=\"Train Loss\");\n","plt.plot(testLossList, label=\"Test Loss\");\n","plt.title(\"Loss\");\n","plt.legend();"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5BZ-S8HBNmqg","colab_type":"text"},"cell_type":"markdown","source":["## Evaluate final model"]},{"metadata":{"id":"uGpA40fHad1I","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","  \n","acc, test_loss = evaluate()\n","\n","print(\"Test accuracy:\" + str(acc) + \" loss: \" + str(test_loss))\n","                   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"l4HywbF3QoBc","colab_type":"text"},"cell_type":"markdown","source":["## Save a model checkpoint. Restoring a model checkpoint"]},{"metadata":{"id":"R2AtYb0uqkYO","colab_type":"code","colab":{}},"cell_type":"code","source":["# Save / restore model\n","\n","#!mkdir ckpts\n","\n","vars_to_save = tf.global_variables()\n","saver = tf.train.Saver(var_list=vars_to_save)\n","\n","\n","model_name ='./ckpts/cifarnet-batchnorm.ckpt'\n","saver.save(sess, model_name, global_step=num_steps)\n","print(vars_to_save)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1qvG9566ju9o","colab_type":"text"},"cell_type":"markdown","source":["## Exmplu calcul acuratete folosint tf.metrics\n","\n","`tf.metrics.accuracy` \n","\n","```\n","labels = ...\n","predictions = ...\n","accuracy, update_op_acc = tf.metrics.accuracy(\n","    labels, predictions)\n","error, update_op_error = tf.metrics.mean_absolute_error(\n","    labels, predictions)\n","\n","sess.run(tf.local_variables_initializer())\n","for batch in range(num_batches):\n","  sess.run([update_op_acc, update_op_error])\n","\n","accuracy, mean_absolute_error = sess.run([accuracy, mean_absolute_error])\n","\n","```\n","\n","https://www.tensorflow.org/api_docs/python/tf/metrics\n"]},{"metadata":{"id":"071zo-MuRBI4","colab_type":"text"},"cell_type":"markdown","source":["## Testing model restore works"]},{"metadata":{"id":"XvHmyUcgueaw","colab_type":"code","colab":{}},"cell_type":"code","source":["# test restore works\n","evaluate()\n","sess.run(init)\n","evaluate()\n","vars_to_restore = tf.global_variables()\n","saver = tf.train.Saver(var_list=vars_to_restore)\n","model_to_restore = \"{}-{}\".format(model_name, num_steps)\n","saver.restore(sess, model_to_restore)\n","evaluate()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y7JLKSjOsQFq","colab_type":"code","colab":{}},"cell_type":"code","source":["#!ls ./ckpts"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qHqvhEvxSCQo","colab_type":"text"},"cell_type":"markdown","source":["### One hot encoding"]},{"metadata":{"id":"cr87Erqjmnge","colab_type":"code","colab":{}},"cell_type":"code","source":["x, y = get_batch(images_test, cls_test, i)\n","\n","y = y[:4]\n","print(y)\n","print(num_classes)\n","\n","res = tf.one_hot(indices=y, depth=num_classes)\n","print(sess.run(res))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2CkNbf7HSI8p","colab_type":"text"},"cell_type":"markdown","source":["### Masurarea performantelor retelei folosind tf.metrics"]},{"metadata":{"id":"6smOZanJj2q_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Remember\n","\n","# predictions = end_points['Predictions']\n","# correct_prediction = tf.equal(tf.argmax(predictions, 1), label_input)\n","# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","# adaugam on nod pentru one hot\n","\n","labels = tf.one_hot(indices=label_input, depth=num_classes)\n","# accuracy_streamed, update_op_acc = tf.contrib.metrics.streaming_accuracy(label_input, tf.argmax(predictions, 1))\n","\n","accuracy_streamed, update_op_acc = tf.metrics.accuracy(label_input, tf.argmax(predictions, 1))\n","\n","# init = tf.global_variables_initializer()\n","sess.run(tf.local_variables_initializer())\n","  \n","\n","vars_to_restore = tf.global_variables()\n","saver = tf.train.Saver(var_list=vars_to_restore)\n","saver.restore(sess, model_to_restore)\n","\n","evaluate()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vKZ7te-wnfDJ","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n","\n","def evaluate_streaming():\n","  total_examples = cls_test.shape[0]\n","  num_batches = int(total_examples / batch_size)\n","  print(\"Total examples {}\".format(total_examples))\n","  print(\"Total iters {}\".format(num_batches))\n","  \n","  for i in range(num_batches):\n","    \n","    x, y = get_batch(images_test, cls_test, i)\n","\n","    feed_dict = {\n","\n","        image_input: x,\n","        label_input: y,\n","        is_training: False\n","\n","    }\n","\n","    #testAcc, testLoss = sess.run([accuracy, loss], feed_dict=feed_dict)\n","    \n","    if i % 10 == 0:\n","      _, test_acc = sess.run([update_op_acc, accuracy_streamed],  feed_dict=feed_dict)\n","      print(\"Test \" + str(i) + \": accuracy:\" + str(test_acc))\n","    \n","    else:\n","      sess.run([update_op_acc], feed_dict=feed_dict)\n","    \n","      \n","  test_acc = sess.run(accuracy_streamed)\n","  print(\"Mean Accuracy  {:.2f} %\".format(test_acc * 100))\n","  \n","  \n","evaluate_streaming()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o60ql2sJ1KDO","colab_type":"text"},"cell_type":"markdown","source":["## Variabile locale vs Variabile globale"]},{"metadata":{"id":"4oCvIEKAxnnO","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.local_variables()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yYmseDp9xry3","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.global_variables()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZppprM7dbSGR","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls ckpts"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JJRbLRU81XHN","colab_type":"text"},"cell_type":"markdown","source":["# Vizualizarea Activarilor din retea"]},{"metadata":{"id":"JVxaAyapZh0G","colab_type":"code","colab":{}},"cell_type":"code","source":["# visualize endpoints\n","end_points\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UnLJw-ndcVqU","colab_type":"code","colab":{}},"cell_type":"code","source":["def plotActivations(units):\n","    filters = units.shape[3]\n","    plt.figure(1, figsize=(20,20))\n","    n_columns = 6\n","    n_rows = math.ceil(filters / n_columns) + 1\n","    for i in range(filters):\n","        plt.subplot(n_rows, n_columns, i+1)\n","        plt.title('Filter ' + str(i))\n","        ax = plt.gca()\n","        ax.grid(False)\n","        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9QXVN9eoblQo","colab_type":"code","colab":{}},"cell_type":"code","source":["conv1 = end_points['conv1']\n","\n","x, y = random_batch(X_train, y_train)\n","\n","feed_dict = {\n","\n","      image_input: x,\n","      label_input: y,\n","      is_training: False\n","\n","  }\n","\n","conv1_fmaps = sess.run(conv1, feed_dict=feed_dict)\n","conv1_fmaps.shape\n","activ1 = conv1_fmaps[0, :, :, :]\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3mAKKqm9dW8i","colab_type":"code","colab":{}},"cell_type":"code","source":["plotActivations(conv1_fmaps)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7vmg1JnhhrT5","colab_type":"text"},"cell_type":"markdown","source":["## Next Steps\n","\n","1. Implementați modulul inception din Fig. 1 în cadrul metodei `inception_module`.\n","\n","2. Implementați arhitectura din Fig.2 în cadrul metodei `cifarnet_bn`. (** Atenție la dimensiunea inputului rețelei ** )\n","  * Ar trebui să obțineți 88% accuracy\n","\n","### Bonus\n","\n","1. Combinați arhitectura de mai sus (Inception) cu conexiunile reziduale (ResNet)\n","  * Hint: [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261.pdf)"]}]}