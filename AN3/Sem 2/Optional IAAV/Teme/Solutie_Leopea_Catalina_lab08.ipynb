{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MareTudorTemaLab_IAAV_2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gOCVAWfbiZn_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "UqxwMxyIgAqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.contrib.slim as slim\n",
        "from google.colab import files\n",
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H222Q72kgH2c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8A3Wv1YAfpMv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "TRAIN_FILE = 'facial_keypoints_train.txt'\n",
        "EVAL_FILE = 'facial_keypoints_eval.txt'\n",
        "LOG_DIR = 'log_dir'\n",
        "IS_TRAINING = False\n",
        "CHECKPOINT_PATH = None\n",
        "CHECKPOINT_NUMBER=79\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "st04XAiDifx_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analiza setului de date"
      ]
    },
    {
      "metadata": {
        "id": "6AgAwY8xifQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(TRAIN_FILE, \"r\") as train_file:\n",
        "  i = 0\n",
        "  for line in train_file:\n",
        "    if i == 5:\n",
        "      break\n",
        "    i += 1\n",
        "    print(line)\n",
        "  \n",
        "    # TODO:\n",
        "    # Afisati pentru primele 5 randuri Coordonatele punctelor si valorile pixelilor\n",
        "    # Se va observa structura datelor in fisier: Fiecare rand contine un exemplu. Primele 30 de valori corespund coordonatelor x si y ale keypoints, iar urmatoarele 9216 valori sunt valorile pixelilor din imaginea flattened\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cj6A7_7iWQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(TRAIN_FILE, \"r\") as train_file:\n",
        "  i = 0\n",
        "  for line in train_file:\n",
        "    if i == 5:\n",
        "      break\n",
        "    i += 1\n",
        "    line=line.split()\n",
        "    plt.figure()\n",
        "    imshow(np.reshape(np.uint8(line[30:]),(96,96)),cmap=\"gray\")\n",
        "    # TODO: Completati codul pentru a afisa primele 5 imagini din setul de antrenare. Tineti cont de faptul ca o imagine in fisier este flattened, iar dimensiunea originala este 96*96 in format grayscale.\n",
        "    # Hint: folositi imshow\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BowJDBymqss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(TRAIN_FILE, \"r\") as train_file:\n",
        "  i = 0\n",
        "  for line in train_file:\n",
        "    if i == 5:\n",
        "      break\n",
        "    i += 1\n",
        "    line = line.split()\n",
        "    \n",
        "    current=np.reshape(np.uint8(line[30:]),(96,96))\n",
        "    plt.figure()\n",
        "    for j in range(1,29,2):\n",
        "      cv2.circle(current,(round(float(line[j+1])),round(float(line[j]))),3,255,1)\n",
        "    imshow(current,cmap=\"gray\")\n",
        "    # TODO: Afisati cele 15 keypoints in primele 5 imagini din setul de antrenare. Primele 15 valori de pe fiecare rand au semnificatia keypoint_1_x, keypoint_1_y, keypoint_2_x, keypoint_2_y...\n",
        "    # Hint: folositi cv2.circle\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Zpgx3XZGFh5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Network"
      ]
    },
    {
      "metadata": {
        "id": "r--cyz06fGjf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "\n",
        "    def __init__(self, inputs, labels, num_outputs, is_training=True):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        self.learning_rate = 0.0002\n",
        "        self.beta1 = 0.9\n",
        "        self.is_training = is_training\n",
        "\n",
        "        self.endpoints = {}\n",
        "\n",
        "        self.create_graph()\n",
        "\n",
        "        if self.is_training:\n",
        "            self.create_loss()\n",
        "            self.build_train_op()\n",
        "            self.create_summaries()\n",
        "\n",
        "    def create_graph(self):\n",
        "        input = tf.reshape(self.inputs, shape=(-1, 96, 96, 1))\n",
        "        endpoints = {}\n",
        "        with tf.variable_scope(\"model_\"):\n",
        "          \n",
        "            # TODO: Creati arhitectura retelei. Tineti cont de faptul ca input-ul este o imagine, iar rezultatul un vector de 30 de valori\n",
        "            # Experimentati diverse arhitecturi si incercati sa obtineti un loss cat mai mic\n",
        "            \n",
        "            net = slim.conv2d(input, 32, [3,3], stride=1, padding ='SAME')\n",
        "            net = slim.max_pool2d(net, [2, 2])\n",
        "            net = slim.conv2d(net,64,[3,3],stride=1,padding='SAME')\n",
        "            net = slim.flatten(net)\n",
        "            net = slim.fully_connected(net, 30, activation_fn=None)\n",
        "\n",
        "            self.endpoints[\"pred\"] = net\n",
        "\n",
        "\n",
        "    def create_loss(self):\n",
        "        with tf.name_scope(\"losses\"):\n",
        "            pred = self.endpoints[\"pred\"]\n",
        "            self.total_loss = tf.losses.mean_squared_error(labels=self.labels, predictions=pred, scope=\"loss_reg\")\n",
        "\n",
        "\n",
        "    def create_summaries(self):\n",
        "        summary_loss = tf.summary.scalar(\"total_loss\", self.total_loss)\n",
        "        summary_lr = tf.summary.scalar(\"learning_rate\", self.optim._lr )\n",
        "        self.summary_op = tf.summary.merge([summary_loss, summary_lr], name='summary_op')\n",
        "\n",
        "    def build_train_op(self):\n",
        "        with tf.name_scope('train_op'):\n",
        "          self.optim = tf.train.AdamOptimizer(self.learning_rate, self.beta1)\n",
        "          self.train_op = slim.learning.create_train_op(self.total_loss, self.optim)\n",
        "          # self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "          # self.incr_global_step = tf.assign(self.global_step, self.global_step + 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGqu6pkSG_9O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Aux"
      ]
    },
    {
      "metadata": {
        "id": "CX5E1JgThQMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_csv(line):\n",
        "\n",
        "    parsed_line = tf.decode_csv(line, [[0.] for _ in range(96 ** 2 + 30)],  field_delim=\" \")\n",
        "    label = parsed_line[:30]\n",
        "\n",
        "    features = parsed_line[30:]\n",
        "    return features, label\n",
        "  \n",
        "\n",
        "def make_initializable_iterator(dataset):\n",
        "  \"\"\"Creates an iterator, and initializes tables.\n",
        "\n",
        "  This is useful in cases where make_one_shot_iterator wouldn't work because\n",
        "  the graph contains a hash table that needs to be initialized.\n",
        "\n",
        "  Args:\n",
        "    dataset: A `tf.data.Dataset` object.\n",
        "\n",
        "  Returns:\n",
        "    A `tf.data.Iterator`.\n",
        "  \"\"\"\n",
        "  iterator = dataset.make_initializable_iterator()\n",
        "  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n",
        "  return iterator\n",
        "\n",
        "def _debug_tf_data(iterator):\n",
        "    # debug the dataset\n",
        "    batch_features, batch_labels = iterator.get_next()\n",
        "    sess = tf.Session()\n",
        "    sess.run(iterator.initializer)\n",
        "    while (True):\n",
        "        r = sess.run([batch_features, batch_labels])\n",
        "        print(r)\n",
        "        break\n",
        "\n",
        "def restore_map():\n",
        "    return {var.op.name: var for var in tf.global_variables()}\n",
        "\n",
        "def get_variables_available_in_checkpoint(variables,\n",
        "                                          checkpoint_path,\n",
        "                                          include_global_step=True):\n",
        "  if isinstance(variables, list):\n",
        "    variable_names_map = {variable.op.name: variable for variable in variables}\n",
        "  elif isinstance(variables, dict):\n",
        "    variable_names_map = variables\n",
        "  else:\n",
        "    raise ValueError('`variables` is expected to be a list or dict.')\n",
        "  ckpt_reader = tf.train.NewCheckpointReader(checkpoint_path)\n",
        "  ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()\n",
        "  if not include_global_step:\n",
        "    ckpt_vars_to_shape_map.pop(tf.GraphKeys.GLOBAL_STEP, None)\n",
        "    ckpt_vars_to_shape_map.pop('train_op/beta1_power', None)\n",
        "    ckpt_vars_to_shape_map.pop('train_op/beta2_power', None)\n",
        "    ckpt_vars_to_shape_map.pop('train_op/global_step', None)\n",
        "  vars_in_ckpt = {}\n",
        "  for variable_name, variable in sorted(variable_names_map.items()):\n",
        "    if variable_name in ckpt_vars_to_shape_map:\n",
        "      if ckpt_vars_to_shape_map[variable_name] == variable.shape.as_list():\n",
        "        vars_in_ckpt[variable_name] = variable\n",
        "      else:\n",
        "        tf.logging.warning('Variable [%s] is available in checkpoint, but has an '\n",
        "                        'incompatible shape with model variable. Checkpoint '\n",
        "                        'shape: [%s], model variable shape: [%s]. This '\n",
        "                        'variable will not be initialized from the checkpoint.',\n",
        "                        variable_name, ckpt_vars_to_shape_map[variable_name],\n",
        "                        variable.shape.as_list())\n",
        "    else:\n",
        "      tf.logging.warning('Variable [%s] is not available in checkpoint',\n",
        "                      variable_name)\n",
        "  if isinstance(variables, list):\n",
        "    return vars_in_ckpt.values()\n",
        "  return vars_in_ckpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJvcuUicHJF0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "id": "Wh979gLahGlS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    file_path = TRAIN_FILE\n",
        "    dataset = tf.data.TextLineDataset(file_path).map(decode_csv)\n",
        "\n",
        "    dataset = dataset.shuffle(buffer_size=512).repeat().batch(BATCH_SIZE)\n",
        "    iterator = make_initializable_iterator(dataset)\n",
        "    batch_features, batch_labels = iterator.get_next()\n",
        "\n",
        "    #_debug_tf_data(iterator)\n",
        "\n",
        "    model = Model(inputs=batch_features, labels=batch_labels, num_outputs=30, is_training=True)\n",
        "    train_tensor = model.train_op\n",
        "\n",
        "\n",
        "    saver = tf.train.Saver(\n",
        "        keep_checkpoint_every_n_hours=1, max_to_keep=20)\n",
        "\n",
        "    if CHECKPOINT_PATH:\n",
        "      var_map = restore_map()\n",
        "      available_var_map = (get_variables_available_in_checkpoint(\n",
        "          var_map, CHECKPOINT_PATH, include_global_step=False))\n",
        "      init_saver = tf.train.Saver(available_var_map)\n",
        "\n",
        "      def initializer_fn(sess):\n",
        "          init_saver.restore(sess, CHECKPOINT_PATH)\n",
        "      init_fn = initializer_fn\n",
        "    else:\n",
        "      init_fn = None\n",
        "\n",
        "    slim.learning.train(\n",
        "        train_tensor,\n",
        "        init_fn=init_fn,\n",
        "        logdir=LOG_DIR,\n",
        "        summary_op=model.summary_op,\n",
        "        number_of_steps=100000,\n",
        "        save_summaries_secs=600,\n",
        "        save_interval_secs=30,\n",
        "        saver=saver)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-RMNFcrHLKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ]
    },
    {
      "metadata": {
        "id": "4ku8SFOthJsC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval(num_steps):\n",
        "    file_path = EVAL_FILE\n",
        "    dataset = tf.data.TextLineDataset(file_path).map(decode_csv)\n",
        "\n",
        "    dataset = dataset.batch(1)\n",
        "    iterator = dataset.make_initializable_iterator()\n",
        "    batch_features, batch_labels = iterator.get_next()\n",
        "\n",
        "    model = Model(inputs=batch_features, labels=batch_labels, num_outputs=30, is_training=False)\n",
        "    errs = []\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(iterator.initializer)\n",
        "        saver.restore(sess, os.path.join(LOG_DIR, f\"model.ckpt-{CHECKPOINT_NUMBER}\"))\n",
        "        for i in range(num_steps):\n",
        "            feat, labels, preds = sess.run([batch_features, batch_labels, model.endpoints['pred']])\n",
        "            err = np.abs(labels - preds)\n",
        "            errs.extend(err[0])\n",
        "\n",
        "        mae = np.mean(np.array(errs))\n",
        "\n",
        "        print(\"MAE: : {}\".format(mae))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Hk0VC4lHNJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Main"
      ]
    },
    {
      "metadata": {
        "id": "Wm4VxNYahK4D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "if IS_TRAINING:\n",
        "    train()\n",
        "else:\n",
        "    eval(num_steps=321)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h246ZcwGHS9h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download checkpoints"
      ]
    },
    {
      "metadata": {
        "id": "vjtoDfioparQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!zip -r logs.zip log_dir\n",
        "files.download('logs.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWIYucddJNZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls log_dir\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VyJJeK_zXba1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fis = open(\"test.txt\",\"w\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ZZhSQSiXggK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}